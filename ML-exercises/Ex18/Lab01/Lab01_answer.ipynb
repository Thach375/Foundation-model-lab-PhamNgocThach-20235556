{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab01-Ans.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7KLk5RJY4uJtrIgAwNnDG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3lJ8UvaQdxwR"},"source":["## Gridworld Simulator\n"]},{"cell_type":"code","metadata":{"id":"0Tx8LT51dpJS","executionInfo":{"status":"ok","timestamp":1630803573423,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"rW6_2kg6d4fW","executionInfo":{"status":"ok","timestamp":1630808290535,"user_tz":-420,"elapsed":308,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}}},"source":["class environment:\n","  def __init__(self, grid_height, grid_width):\n","    \"\"\"\n","    The map is initialized with height and width are varible of your choice\n","    start: List of location where you step in, you get to the corresponding location in list 'end'\n","           For example:\n","              if you step in location start[3] then you get to new location end[3] then obtain the reward value of reward[3]\n","    reward: List of reward value where you move from a location in 'start' list to the corresponding location in 'end' list\n","    \"\"\"\n","    self.height = grid_height\n","    self.width = grid_width\n","    self.start = []\n","    self.end = []\n","    self.reward = []\n","    self.map = np.array([i for i in range(grid_height * grid_width)])\n","    self.action_space = [0,1,2,3]\n","    \n","  def get_Map(self):\n","    print(self.map.reshape([self.width, self.height]))\n","\n","  def get_NumState(self):\n","    return self.height * self.width\n","\n","  def map_Designate(self, start_cell, end_cell, reward):\n","    self.start.append(start_cell)\n","    self.end.append(end_cell)\n","    self.reward.append(reward)\n","  \n","  def get_Observation(self, location, action):\n","    # If the agent at special locations, all action lead to a single location, gain reward\n","    if location in self.start:\n","      idx = self.start.index(location)\n","      new_location = self.end[idx]\n","      reward = self.reward[idx]\n","      return new_location, self.action_space, reward\n","\n","    # If the agent not at special locations, reward = 0\n","    reward = 0\n","    new_location = 0\n","    # Action: UP: 0, DOWN: 1, LEFT: 2, RIGHT: 3\n","    # Actions that get the agent out of the map, result in no change at all\n","    if action == 0: #UP\n","      if location - self.width < 0:\n","        new_location = location\n","      else:\n","        new_location = location - self.width\n","    \n","    elif action == 1: #DOWN\n","      if location + self.width > self.height * self.width - 1:\n","        new_location = location\n","      else:\n","        new_location = location + self.width\n","\n","    elif action == 2: #LEFT\n","      if location % self.width == 0:\n","        new_location = location \n","      else:\n","        new_location = location - 1\n","\n","    elif action == 3: #RIGHT\n","      if (location + 1) % self.width == 0:\n","        new_location = location \n","      else:\n","        new_location = location + 1\n","        \n","    return new_location, self.action_space, reward"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g39pcvXxeBfl"},"source":["## Grid map design"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoV7B5wid-vc","executionInfo":{"status":"ok","timestamp":1630808297496,"user_tz":-420,"elapsed":331,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}},"outputId":"92876311-d422-44ec-b845-17d92384f13c"},"source":["#Environment setup\n","Envir = environment(8,8)\n","Envir.get_Map()\n","Envir.map_Designate(17,56,-15)\n","Envir.map_Designate(18,56,-15)\n","Envir.map_Designate(19,56,-15)\n","Envir.map_Designate(21,56,-15)\n","Envir.map_Designate(25,56,-15)\n","Envir.map_Designate(33,56,-15)\n","Envir.map_Designate(41,56,-15)\n","Envir.map_Designate(42,56,-15)\n","Envir.map_Designate(43,56,-15)\n","Envir.map_Designate(46,56,-15)\n","Envir.map_Designate(47,56,-15)\n","Envir.map_Designate(47,56,-15)\n","Envir.map_Designate(15,56,+15)\n","Envir.map_Designate(1,10,+5)\n","Envir.map_Designate(26,56,+20)\n","\n","# Check for the start, end, reward lists\n","for i in range(len(Envir.start)):\n","  print('i = '+ str(i) + '|Start at ' + str(Envir.start[i]) + ' results at ' + str(Envir.end[i]) + ' get Reward: ' + str(Envir.reward[i]))"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  1  2  3  4  5  6  7]\n"," [ 8  9 10 11 12 13 14 15]\n"," [16 17 18 19 20 21 22 23]\n"," [24 25 26 27 28 29 30 31]\n"," [32 33 34 35 36 37 38 39]\n"," [40 41 42 43 44 45 46 47]\n"," [48 49 50 51 52 53 54 55]\n"," [56 57 58 59 60 61 62 63]]\n","i = 0|Start at 17 results at 56 get Reward: -15\n","i = 1|Start at 18 results at 56 get Reward: -15\n","i = 2|Start at 19 results at 56 get Reward: -15\n","i = 3|Start at 21 results at 56 get Reward: -15\n","i = 4|Start at 25 results at 56 get Reward: -15\n","i = 5|Start at 33 results at 56 get Reward: -15\n","i = 6|Start at 41 results at 56 get Reward: -15\n","i = 7|Start at 42 results at 56 get Reward: -15\n","i = 8|Start at 43 results at 56 get Reward: -15\n","i = 9|Start at 46 results at 56 get Reward: -15\n","i = 10|Start at 47 results at 56 get Reward: -15\n","i = 11|Start at 47 results at 56 get Reward: -15\n","i = 12|Start at 15 results at 56 get Reward: 15\n","i = 13|Start at 1 results at 10 get Reward: 5\n","i = 14|Start at 26 results at 56 get Reward: 20\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xbl5N52Mv90s","executionInfo":{"status":"ok","timestamp":1630808433346,"user_tz":-420,"elapsed":284,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}},"outputId":"f153f7c3-ed6b-42fa-a9fb-22dabad2bb4c"},"source":["# Test environment\n","a, b, r = Envir.get_Observation(1,0)\n","print(r)"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]}]},{"cell_type":"markdown","metadata":{"id":"-OzOYHRmeKmF"},"source":["## MAB Agent"]},{"cell_type":"code","metadata":{"id":"QVSTPnCpeHVB","executionInfo":{"status":"ok","timestamp":1630808551506,"user_tz":-420,"elapsed":313,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}}},"source":["class MAB_agent:\n","  def __init__(self, envir, init_location):\n","    # Trace the reward\n","    self.reward_trace = []\n","    # initialize the first location\n","    self.location_now = init_location\n","    # TODO: implement other features to the agent so it can perform MAB algorithm\n","    self.lastAction = None\n","    self.lastState = None\n","    self.value_table = {}    # format: {state : {action : [value, count]}}\n","\n","  def get_TotalReward(self):\n","    return np.sum(self.reward_trace)\n","\n","  # Running in Simulator\n","  def getAction(self, observation): \n","    self.location_now, action_space, pre_reward = observation\n","    # NOTICE: the first observation is (init_location, [0,1,2,3], None)\n","    # You should process the 'None' \n","\n","    if self.location_now not in self.value_table.keys():\n","      # if the state has not been observed before, add to the table with [value, count] = [0, 1]\n","      self.value_table[self.location_now] = {i: [0, 1] for i in action_space}\n","\n","    if pre_reward is None:\n","      action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n","    else:\n","      self.reward_trace.append(pre_reward)\n","      # Updating with incremental average over reward samples\n","      value = self.value_table[self.lastState][self.lastAction][0]\n","      count = self.value_table[self.lastState][self.lastAction][1]\n","\n","      count += 1\n","      value += (1/count) * (pre_reward - value)\n","\n","      self.value_table[self.lastState][self.lastAction][0] = value\n","      self.value_table[self.lastState][self.lastAction][1] = count\n","\n","      # get action\n","      state_dict = self.value_table[self.lastState].values()\n","      state_dict_array = np.array(list(state_dict))\n","      value_column = state_dict_array[:,0]\n","      action = np.argmax(value_column)\n","\n","    self.lastState = self.location_now\n","    self.lastAction = action\n","    # Assert valid action\n","    assert action in action_space, \"INVALID action taken\"\n","    return action"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJNT0UMtgpYE"},"source":["# create your agent, with environment is the pre-declared Envir and init_location set to 0\n","init_location = 0\n","dummyAgent = MAB_agent(envir=Envir, init_location=init_location)\n","\n","num_iter = 100\n","log_freq = 10\n","Data_plot1 = []\n","Action_record = []\n","\n","for i in range(num_iter):\n","  env_observation = (init_location, Envir.action_space, None)\n","  if i > 0:\n","    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n","\n","  chosen_action = dummyAgent.getAction(observation=env_observation)\n","  Action_record.append(chosen_action)\n","  if (i + 1) % log_freq == 0:\n","    aver = np.mean(dummyAgent.reward_trace)\n","    Data_plot1.append(aver)\n","    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_IA2ilhn5SG"},"source":["## MABe Agent"]},{"cell_type":"code","metadata":{"id":"z3ZJC6b5n3se","executionInfo":{"status":"ok","timestamp":1630808573723,"user_tz":-420,"elapsed":310,"user":{"displayName":"Hưng Nguyễn Năng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64","userId":"17472566638549226668"}}},"source":["class MABe_agent(MAB_agent):\n","  def __init__(self, envir, init_location, epsilon):\n","    super(MABe_agent, self).__init__(envir, init_location)\n","    self.epsilon = epsilon\n","\n","  # Override\n","  def getAction(self, observation): \n","    self.location_now, action_space, pre_reward = observation\n","    # NOTICE: the first observation is (init_location, [0,1,2,3], None)\n","    # You should process the 'None' \n","    if self.location_now not in self.value_table.keys():\n","      # if the state has not been observed before, add to the table with [value, count] = [0, 1]\n","      self.value_table[self.location_now] = {i: [0, 1] for i in action_space}\n","    \n","    toss = np.random.rand()\n","\n","    if pre_reward is None or toss < self.epsilon:\n","      action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n","    else:\n","      self.reward_trace.append(pre_reward)\n","      # Updating with incremental average over reward samples\n","      value = self.value_table[self.lastState][self.lastAction][0]\n","      count = self.value_table[self.lastState][self.lastAction][1]\n","\n","      count += 1\n","      value += (1/count) * (pre_reward - value)\n","\n","      self.value_table[self.lastState][self.lastAction][0] = value\n","      self.value_table[self.lastState][self.lastAction][1] = count\n","      \n","      # get action\n","      state_dict = self.value_table[self.lastState].values()\n","      state_dict_array = np.array(list(state_dict))\n","      value_column = state_dict_array[:,0]\n","      action = np.argmax(value_column)\n","\n","    self.lastState = self.location_now\n","    self.lastAction = action\n","    # Assert valid action\n","    assert action in action_space, \"INVALID action taken\"\n","    return action"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"MD7QK77-pdWg"},"source":["# Run your MABe agent\n","# create your agent, with environment is the pre-declared Envir and init_location set to 0\n","init_location = 0\n","epsilon=0.5\n","dummyAgent = MABe_agent(envir=Envir, init_location=init_location, epsilon=epsilon)\n","\n","num_iter = 1000\n","log_freq = 100\n","Data_plot2 = []\n","Action_record = []\n","Location_record = []\n","\n","for i in range(num_iter):\n","  env_observation = (init_location, Envir.action_space, None)\n","  if i > 0:\n","    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n","\n","  chosen_action = dummyAgent.getAction(observation=env_observation)\n","  Location_record.append(env_observation[0])\n","  Action_record.append(chosen_action)\n","\n","  if (i + 1) % log_freq == 0:\n","    aver = np.mean(dummyAgent.reward_trace)\n","    Data_plot2.append(aver)\n","    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))"],"execution_count":null,"outputs":[]}]}